# RAG_With_PDF-FullStack

Experimenting with Langchain, DataStax and Streamlit to build Full Stack app that allows users to directly chat with PDFs. Currently working with only using open source models to further understanding of open source models and their capabilities. All models used are from Hugging Face. Currently, open source models are run locally, which means that the model restarts for every user. Looking to add containers using Docker that reduce latency and allow for the model to always active for the user

<h2> Future Work</h2>

1. Adding more models that are availible on Hugging Face
2. Optimizing latency between query and output
3. Hosting website on the cloud

<h2> Picture of interface</h2>

<img width="1919" height="1023" alt="Screenshot 2025-08-25 002742" src="https://github.com/user-attachments/assets/9e563dcd-866e-42a5-a1e8-c7dfef47ec23" />
